# ğŸ“š **Explainify â€“ Text-to-Visual Learning AI**

**ğŸš¨ *Problem Statement*** 

Learning complex topics from textbooks, lecture notes, or research papers can be challenging and time-consuming ğŸ˜©. Most resources rely heavily on text, which:

Can be hard to understand for visual learners ğŸ§ ğŸ‘€

Leads to low retention of important concepts ğŸ“‰

Requires students to spend extra time summarizing and visualizing key points â³

Thereâ€™s a need for a smart, automated solution that can convert textual lessons into visually engaging, easy-to-understand videos ğŸ¬âœ¨ â€” making learning faster, fun, and more effective.

**ğŸ’¡ *Initial Solution***

Explainify transforms any text lesson into a visual + audio learning experience. Hereâ€™s how it works:

**ğŸ“*Text Processing***

Input: any lesson, paragraph, or notes.

Automatically splits text into key scenes or sentences using NLP ğŸ”.

**ğŸ–¼ *Visual Generation***

Generates realistic AI images for each scene (DALLÂ·E, Stable Diffusion) ğŸ¨ğŸ¤–

Adds animations like zoom-in, fade, or slide transitions âœ¨ğŸ

**ğŸ¤ *Audio Narration***

Converts text to speech using TTS technology ğŸ”Š

Synchronizes narration with visuals for smooth comprehension ğŸ§

**ğŸ¬ *Video Composition***

Combines images, audio, and animations into a ready-to-watch video ğŸ–¥ğŸ“¹

Adds titles, captions, and subtitles to highlight key concepts ğŸ·ğŸ“Œ

***Outcome:***
A fully automated educational video that turns boring text into a dynamic, engaging learning experience ğŸš€ğŸ’¡.
Students can understand faster, remember longer, and enjoy learning like never before ğŸ¯ğŸ“–âœ¨

ğŸŒŸ **How Explainify Works â€“ Visual Flow**

ğŸ“„ Text Lesson / Notes

        â”‚
        â–¼
        
ğŸ§  NLP / Text Processing
   (Split into scenes / key sentences)
   
        â”‚
        â–¼
        
ğŸ–¼ AI Image Generation
   (DALLÂ·E / Stable Diffusion / Stock Images)
   
        â”‚
        â–¼
        
ğŸ¤ Text-to-Speech Audio
   (Narration for each scene)
   
        â”‚
        â–¼
        
ğŸ Video Composition
   (Animations, transitions, titles, captions)

        â”‚
        â–¼
        
ğŸš€ Final Explainify Video
   (Engaging, visual + audio learning experience)
   
        â”‚
        â–¼
        
ğŸ“š Students Understand Faster & Retain Longer

## ğŸ‘¥ Team Roles and Work Allocation

***1ï¸âƒ£ Text Processing & NLP***

Role: Convert raw text into structured scenes
Tasks:

Take the input lesson/text.

Split the text into scenes or sentences using NLP (nltk / spacy).

Optionally summarize or highlight key points per scene.

Output scene-wise text for image and audio generation.

Tools:

Python, NLTK (sent_tokenize), SpaCy (optional), Pandas (optional for scene storage)

Deliverables:

A list of scene texts ready for images and narration.

Example: ["Photosynthesis converts sunlight into energy", "It occurs in chloroplasts", ...]



***2ï¸âƒ£ Image Generation***

Role: Generate visual representation for each scene
Tasks:

Take scene texts from Member 1.

Use AI image generation or free stock images.

Options: OpenAI DALLÂ·E, Stable Diffusion, Unsplash API

Save images locally (scene_1.png, scene_2.png, etc.)

Optional: Apply simple overlays or text highlights on images.

Tools:

Python, OpenAI API, Requests, PIL (Pillow)

Image editing software (optional)

Deliverables:

Scene images ready for video composition

***3ï¸âƒ£ Audio Narration + Scene Video Clips***

Role: Create audio narration and merge with images for each scene
Tasks:

Take scene texts from Member 1.

Generate TTS audio for each scene (gTTS or ElevenLabs).

Combine scene image + narration into a short video clip.

Add scene titles / captions / small animations (zoom, fade, crossfade).

Tools:

Python, gTTS, MoviePy, PIL (for text overlays)

Deliverables:

Individual scene video clips (scene_1.mp4, scene_2.mp4, â€¦)

***4ï¸âƒ£ Video Compilation & Final Rendering***

1.Role: Combine all scene clips into a final video
Tasks:

Take all scene clips from Member 3.

Apply transitions between scenes (crossfade, fade-in/out).

Add background music or final audio enhancements (optional).

Export final Explainify video (explainify_ai_video.mp4).

Tools:

Python, MoviePy, FFMPEG (optional)

Deliverables:

Final complete video ready for download or presentation

***ğŸ§  How It Works (Step-by-Step)***

## Step-1:ğŸ§¾ Text Extraction from Multiple File Formats

This Google Colab project allows you to upload a file and automatically extract text from various file formats â€” including .txt, .pdf, .docx, .csv, .json, .pptx, and even image files (.png, .jpg, .jpeg) using OCR.

â€¢	***Upload the file:***
The script uses google.colab.files.upload() to let you upload any supported file.

â€¢	***Detect file type:***
Based on the file extension, it decides which extraction method or library to use.

â€¢	***Extract text:***
Reads or processes the file:

           .txt: Simple read

           .pdf: Uses PyMuPDF (fitz)

           .docx: Uses python-docx

           .csv: Uses pandas

           .json: Uses json and pprint

           .pptx: Uses python-pptx

           .png / .jpg / .jpeg: Uses pytesseract and Pillow for OCR

â€¢	***Display the extracted text:***
Shows the extracted text inside a formatted HTML block for easy reading.

Displays a message if no text is found.

## ğŸš€ Features:

ğŸ“„ Extract text from Word, PDF, and TXT files

ğŸ“Š Read and display CSV and JSON data

ğŸï¸ Capture text from PowerPoint (.pptx) slides

ğŸ–¼ï¸ Perform OCR (Optical Character Recognition) on images

âš™ï¸ Automatically detects file type and uses the right extraction method

ğŸ§  Built for Google Colab â€” no local setup required
## Step-2: ğŸ¬ AI Explainer Video Pipeline (Female Voice) â€“ Colab

Create *automated explainer videos* from documents or images with a *female voice narration* â€” fully in *Google Colab*!  

---

## âœ¨ Features
- ğŸ“„ Supports multiple file types: .txt, .pdf, .docx, .csv, .json, .pptx, .png, .jpg, .jpeg  
- ğŸ“ Summarizes long text using *Facebook BART Transformer*  
- ğŸ¤ Generates *female-like voice narration* using *gTTS*  
- ğŸ–Œ Creates a simple *background image* for the video  
- ğŸ¥ Combines *audio + image* into a downloadable .mp4 video  
- â¬‡ Direct download link in Colab  

---

## ğŸš€ Workflow
1. *Upload File* â€“ Upload your document or image ğŸ“  
2. *Extract Text* â€“ Automatically detects file type and extracts content âœ‚  
3. *Verify Text* â€“ Displays extracted content for review ğŸ‘€  
4. *Summarize Text* â€“ Creates a concise summary using AI ğŸ¤–  
5. *Convert to Audio* â€“ Generates female voice narration ğŸ§  
6. *Create Background Image* â€“ Adds a simple colored background with text ğŸ–¼  
7. *Generate Video* â€“ Combines image + audio into .mp4 video ğŸ¬  
8. *Download Video* â€“ Provides a direct download link ğŸ’¾  

---

## ğŸ“¦ Dependencies
- transformers, torch â€“ Text summarization ğŸ§   
- gTTS â€“ Text-to-speech conversion ğŸ¤  
- moviepy â€“ Video creation ğŸ  
- pillow â€“ Image processing ğŸ¨  
- pymupdf, python-docx, python-pptx, pytesseract â€“ Text extraction from files ğŸ—‚  

---

## ğŸ“ Usage
1. Open the notebook in *Google Colab* ğŸŒ  
2. Upload your file when prompted ğŸ“¤  
3. Wait for the pipeline to extract, summarize, generate audio, and create the video â³  
4. Download your final *explainer video* ğŸ¥â¬‡  

---
## Step-3: Image Generation
ğŸ§  Description of the Work

This code uses Stable Diffusion, a powerful AI image generation model, to create realistic images from text prompts using Python.

ğŸ” **Step-by-Step Explanation**

Installing Dependencies

- *!pip install -q diffusers transformers accelerate torch pillow*


Installs all the required libraries:

diffusers â†’ to load and run diffusion models

transformers â†’ for text encoding

accelerate â†’ for efficient GPU computation

torch â†’ PyTorch framework for deep learning

pillow â†’ for image processing and saving

Importing Required Modules

- *from diffusers import StableDiffusionPipeline
import torch*


Imports the AI model pipeline (StableDiffusionPipeline)

Imports PyTorch for GPU and tensor operations

Checking GPU Availability

- *print("GPU Available:", torch.cuda.is_available())*


Confirms whether a CUDA-enabled GPU is available for faster image generation.

Loading the Pre-trained Model

- *pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    safety_checker=None
).to("cuda")*


Loads the Stable Diffusion v1.5 model (by RunwayML).

Uses half precision (float16) for faster inference.

Moves the model to the GPU for acceleration.

Disables safety checker (optional in local usage).

Generating the Image

- *prompt = "dogs"
- image = pipe(prompt, num_inference_steps=30).images[0]*


The model converts the text prompt into an image.

num_inference_steps controls the quality (more steps â†’ higher quality).

Displaying and Saving the Output

- *display(image)
- image.save("my_image.png")*


Shows the generated image in the output window.

Saves it locally as my_image.png.

ğŸ¨ **Example Output**

If the prompt is:

"Dogs"

The model will create a scientific visual showing dogs in the picture.

âš™ï¸ **Purpose**

This demonstrates how text-to-image generation works using AI models. It can be used for:

- Educational visualizations

- Creative content generation

- Design prototypes

- Data visualization in AI projects
